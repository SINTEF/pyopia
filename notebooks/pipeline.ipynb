{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from pyopia.classify import Classify\n",
    "import exampledata\n",
    "import pyopia.io\n",
    "from pyopia.pipeline import Pipeline\n",
    "import pyopia.instrument.silcam\n",
    "import pyopia.process\n",
    "import pyopia.background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = exampledata.get_example_silc_image()\n",
    "\n",
    "files = glob('raw_data/*.silc') # relies on having a list of raw files - not currently auto-obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising pipeline\n",
      "  Running <pyopia.classify.Classify object at 0x000002AF25055CA0>\n",
      "  Running <pyopia.background.CreateBackground object at 0x000002AF4155B7C0>\n",
      "Pipeline ready with these data:  ['cl', 'bgstack', 'imbg']\n"
     ]
    }
   ],
   "source": [
    "# example future pre-processing admin:\n",
    "# 1) config file config load\n",
    "# 2) build steps dict from loaded file\n",
    "# 3) establish non-processing related metadata\n",
    "\n",
    "os.makedirs('proc', exist_ok=True)\n",
    "\n",
    "datafile_hdf = 'proc/test'\n",
    "model_path = exampledata.get_example_model()\n",
    "threshold = 0.85\n",
    "\n",
    "average_window = 10  # number of images to use as background\n",
    "bgfiles = files[:average_window]  # provide a list of background files used to create the static background\n",
    "\n",
    "steps = {'classifier': Classify(model_path=model_path),\n",
    "         'create background': pyopia.background.CreateBackground(bgfiles,\n",
    "                                                                 pyopia.instrument.silcam.load_image),\n",
    "         'load': pyopia.instrument.silcam.SilCamLoad(),\n",
    "         'correct background': pyopia.background.CorrectBackgroundAccurate(),  # pyopia.background.shift_bgstack_accurate\n",
    "         'imageprep': pyopia.instrument.silcam.ImagePrep(image_level='imc'),\n",
    "         'segmentation': pyopia.process.Segment(threshold=threshold),\n",
    "         'statextract': pyopia.process.CalculateStats(export_outputpath='export'),\n",
    "         'output': pyopia.io.StatsH5(datafile_hdf)}\n",
    "\n",
    "processing_pipeline = Pipeline(steps,\n",
    "                               initial_steps=['classifier', 'create background'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load a pre-defined set of steps for silcam analysis, you can do it like this:\n",
    "from pyopia.instrument.silcam import silcam_steps\n",
    "default_steps, default_initial_steps = silcam_steps(model_path, threshold, datafile_hdf)\n",
    "# then you would initialise the pipeline like this:\n",
    "# processing_pipeline = Pipeline(default_steps, initial_steps=default_initial_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising pipeline\n",
      "  Running <pyopia.classify.Classify object at 0x000002AF25055CA0>\n",
      "  Running <pyopia.background.CreateBackground object at 0x000002AF4155B7C0>\n",
      "Pipeline ready with these data:  ['cl', 'bgstack', 'imbg']\n",
      "calling:  <class 'pyopia.instrument.silcam.SilCamLoad'>  with:  ['cl', 'bgstack', 'imbg', 'filename', 'steps_string']\n",
      "calling:  <class 'pyopia.background.CorrectBackgroundAccurate'>  with:  ['cl', 'bgstack', 'imbg', 'filename', 'steps_string', 'timestamp', 'imraw']\n",
      "calling:  <class 'pyopia.instrument.silcam.ImagePrep'>  with:  ['cl', 'bgstack', 'imbg', 'filename', 'steps_string', 'timestamp', 'imraw', 'imc']\n",
      "calling:  <class 'pyopia.process.Segment'>  with:  ['cl', 'bgstack', 'imbg', 'filename', 'steps_string', 'timestamp', 'imraw', 'imc', 'imref']\n",
      "segment\n",
      "clean\n",
      "calling:  <class 'pyopia.process.CalculateStats'>  with:  ['cl', 'bgstack', 'imbg', 'filename', 'steps_string', 'timestamp', 'imraw', 'imc', 'imref', 'imbw']\n",
      "statextract_light\n",
      "0.1% saturation\n",
      "measure\n",
      "  35 particles found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"conv2d_6\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (1, 32, 32)\n\nCall arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 32, 32), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m processing_pipeline \u001b[39m=\u001b[39m Pipeline(steps, initial_steps\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcreate background\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m files:\n\u001b[1;32m----> 3\u001b[0m     stats \u001b[39m=\u001b[39m processing_pipeline\u001b[39m.\u001b[39;49mrun(filename)\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\pipeline.py:142\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcalling: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[s])), \u001b[39m'\u001b[39m\u001b[39m with: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[s](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[0;32m    144\u001b[0m stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m stats\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\process.py:624\u001b[0m, in \u001b[0;36mCalculateStats.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m    623\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstatextract_light\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 624\u001b[0m     stats, saturation \u001b[39m=\u001b[39m statextract_light(data[\u001b[39m'\u001b[39;49m\u001b[39mimbw\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mimc\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mcl\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    625\u001b[0m                                           max_coverage\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_coverage,\n\u001b[0;32m    626\u001b[0m                                           max_particles\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_particles,\n\u001b[0;32m    627\u001b[0m                                           export_outputpath\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexport_outputpath,\n\u001b[0;32m    628\u001b[0m                                           min_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_length)\n\u001b[0;32m    629\u001b[0m     stats[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    630\u001b[0m     stats[\u001b[39m'\u001b[39m\u001b[39msaturation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m saturation\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\process.py:402\u001b[0m, in \u001b[0;36mstatextract_light\u001b[1;34m(imbw, timestamp, imc, Classification, max_coverage, max_particles, export_outputpath, min_length)\u001b[0m\n\u001b[0;32m    399\u001b[0m     imc[:, :, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m imref\n\u001b[0;32m    400\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mWARNING. exportparticles temporarily modified for 2-d images without color!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 402\u001b[0m stats \u001b[39m=\u001b[39m extract_particles(imc, timestamp, Classification, region_properties,\n\u001b[0;32m    403\u001b[0m                           export_outputpath\u001b[39m=\u001b[39;49mexport_outputpath, min_length\u001b[39m=\u001b[39;49mmin_length)\n\u001b[0;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m stats, saturation\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\process.py:269\u001b[0m, in \u001b[0;36mextract_particles\u001b[1;34m(imc, timestamp, Classification, region_properties, export_outputpath, min_length)\u001b[0m\n\u001b[0;32m    265\u001b[0m             HDF5File\u001b[39m.\u001b[39mcreate_dataset(\u001b[39m'\u001b[39m\u001b[39mPN\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i), data\u001b[39m=\u001b[39mroi)\n\u001b[0;32m    266\u001b[0m             \u001b[39m# @todo also include particle stats here too.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \n\u001b[0;32m    268\u001b[0m         \u001b[39m# run a prediction on what type of particle this might be\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m         prediction \u001b[39m=\u001b[39m Classification\u001b[39m.\u001b[39;49mproc_predict(roi)\n\u001b[0;32m    270\u001b[0m         predictions[\u001b[39mint\u001b[39m(i), :] \u001b[39m=\u001b[39m prediction[\u001b[39m0\u001b[39m]\n\u001b[0;32m    272\u001b[0m \u001b[39mif\u001b[39;00m export_outputpath \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[39m# close the HDF5 file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\classify.py:124\u001b[0m, in \u001b[0;36mClassify.proc_predict\u001b[1;34m(self, img_input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39mRun pre-processing (:meth:`Classify.preprocessing`) and prediction (:meth:`Classify.predict`)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39musing tensorflow model to classify particles. example here based on the pysilcam network setup.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m    prediction (array) : the probability of the roi belonging to each class\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    123\u001b[0m img_preprocessed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessing(img_input)\n\u001b[1;32m--> 124\u001b[0m prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(img_preprocessed)\n\u001b[0;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32mc:\\users\\emlynd\\documents\\github\\pyopia\\pyopia\\classify.py:107\u001b[0m, in \u001b[0;36mClassify.predict\u001b[1;34m(self, img_preprocessed)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, img_preprocessed):\n\u001b[0;32m     96\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m    Use tensorflow model to classify particles. example here based on the pysilcam network setup.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m        prediction (array)       : the probability of the roi belonging to each class\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(np\u001b[39m.\u001b[39;49mexpand_dims(img_preprocessed, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32mc:\\Users\\emlynd\\.conda\\envs\\pyopia\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\emlynd\\.conda\\envs\\pyopia\\lib\\site-packages\\keras\\engine\\input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    257\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"conv2d_6\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (1, 32, 32)\n\nCall arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 32, 32), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "processing_pipeline = Pipeline(steps, initial_steps=['classifier', 'create background'])\n",
    "for filename in files:\n",
    "    stats = processing_pipeline.run(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metadata in the h5\n",
    "pyopia.io.show_h5_meta(datafile_hdf + '-STATS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopia.statistics\n",
    "# load the stats DataFrame from the h5 file\n",
    "stats = pd.read_hdf(datafile_hdf + '-STATS.h5', 'ParticleStats/stats')\n",
    "print('stats header: ', stats.columns)\n",
    "print('Total number of particles: ', len(stats))\n",
    "print('Number of raw images: ', pyopia.statistics.count_images_in_stats(stats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35ab0c005d63a5587fede8db5b2b16b081d9aece903d58f7211748c218ea86a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
