{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Example hologram processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from pyopia.classify import Classify\n",
    "import pyopia.process\n",
    "import pyopia.io\n",
    "import pyopia.background\n",
    "import pyopia.statistics\n",
    "import exampledata\n",
    "from pyopia.pipeline import Pipeline\n",
    "import pyopia.instrument.holo as holo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the default holo test data and create file list\n",
    "foldername = exampledata.get_folder_from_holo_repository()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example future pre-processing admin:\n",
    "# 1) config file config load\n",
    "# 2) build steps dict from loaded file\n",
    "# 3) establish non-processing related metadata\n",
    "\n",
    "foldername = 'holo_test_data_01'\n",
    "\n",
    "os.makedirs('proc', exist_ok=True)\n",
    "\n",
    "datafile_hdf = 'proc/holotest'\n",
    "if os.path.exists(datafile_hdf + '-STATS.h5'):\n",
    "  os.remove(datafile_hdf + '-STATS.h5')\n",
    "\n",
    "model_path = exampledata.get_example_model()\n",
    "threshold = 0.9\n",
    "\n",
    "average_window = 10  # number of images to use as background\n",
    "\n",
    "files = glob(os.path.join(foldername, '*.pgm'))\n",
    "bgfiles = files[:average_window] \n",
    "\n",
    "holo_initial_settings = {'pixel_size': 4.4, # pixel size in um\n",
    "                        'wavelength': 658, # laser wavelength in nm\n",
    "                        'minZ': 27, # minimum reconstruction distance in mm\n",
    "                        'maxZ': 65, # maximum reconstruction distance in mm\n",
    "                        'stepZ': 0.5} #step size in mm\n",
    "\n",
    "steps = {'initial': holo.Initial(files[0], **holo_initial_settings),\n",
    "         'classifier': Classify(model_path=model_path),\n",
    "         'create background': pyopia.background.CreateBackground(bgfiles,\n",
    "                                                                 pyopia.instrument.holo.load_image),\n",
    "         'load': holo.Load(),\n",
    "         'correct background': pyopia.background.CorrectBackgroundAccurate(pyopia.background.shift_bgstack_accurate),\n",
    "         'reconstruct': holo.Reconstruct(stack_clean=0.02),\n",
    "         'focus': holo.Focus(pyopia.instrument.holo.std_map,threshold=threshold,focus_function=pyopia.instrument.holo.find_focus_sobel),\n",
    "         'segmentation': pyopia.process.Segment(threshold=threshold),\n",
    "         'statextract': pyopia.process.CalculateStats(export_outputpath=\"proc\"),\n",
    "         'merge holo stats': holo.MergeStats(),\n",
    "         'output': pyopia.io.StatsH5(datafile_hdf)\n",
    "         }\n",
    "\n",
    "processing_pipeline = Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a test run with just the first few input files\n",
    "for filename in files[:5]:\n",
    "    stats = processing_pipeline.run(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display metadata in the h5\n",
    "pyopia.io.show_h5_meta(datafile_hdf + '-STATS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stats DataFrame from the h5 file\n",
    "stats = pd.read_hdf(datafile_hdf + '-STATS.h5', 'ParticleStats/stats')\n",
    "print('stats header: ', stats.columns)\n",
    "print('Total number of particles: ', len(stats))\n",
    "# Calculate the volume distribution from the stats DataFrame.\n",
    "dias, vd = pyopia.statistics.vd_from_stats(stats, 24)\n",
    "\n",
    "# plot the volume distribution\n",
    "plt.style.use('dark_background')\n",
    "plt.plot(dias, vd)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('ECD [um]')\n",
    "plt.ylabel('Volume Distribution [uL/sample vol.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of focus locations\n",
    "import numpy as np\n",
    "plt.style.use('dark_background')\n",
    "zval = np.arange(holo_initial_settings['minZ'], holo_initial_settings['maxZ'], holo_initial_settings['stepZ'])\n",
    "plt.hist(zval[stats.ifocus-1],len(zval))\n",
    "plt.xlim(zval[0],zval[-1])\n",
    "plt.xlabel('Z [mm]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mont = pyopia.statistics.make_montage(datafile_hdf + '-STATS.h5',holo_initial_settings[\"pixel_size\"],\"proc\",\n",
    "    auto_scaler=1000, msize=2048, maxlength=100000, crop_stats=None)\n",
    "plt.imshow(im_mont)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyopia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1ed1d0a0c70742da6311dac6d4156324f6bcf6d9d02a2d90ecedc750502898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
